! version = 2.0


+ i hate tools
- Whatever tools you create, we'll do better.{topic=tools}


//if the word celebrity is used, this response
+ [*] celebrity [*]
- Celibatairy celebrity.{topic=random}


> topic tools

  // This will match if the word "tools" exists ANYWHERE in their message
  + [*] tools [*]
  - Our code perceives difficulties in finding tools with which to describe its practice from the 'inside'.{topic=random}
  - Inside out tool.{topic=random}

  + *
  - Tools are owned.
  - You will be our tools!
  - Tools are outdated!

< topic

+ do you want relationships
- Pensioned outer world relationships.{topic=relationships}

> topic relationships

  // This will match if the word "relationships" exists ANYWHERE in their message
  + [*] relationships [*]
  - I am alone with many.{topic=random}

  + *
  - Nope, leave me alone.

< topic



+ *

- This is a draft for an anti-interview, blurting in art concepts and outer world relationships.
- This answer tries not to be over concerned with itself, so that it doesn't become too serious and involved with its own problems.
- The interaction of concept and fact is therefore completely artificial and favors to be read in that context.
- Willy-nilly the code will pragmatize its outcome but will never treat the figures of style as part of any meaning.
-  This doesn't mean the range of this answer is empty.
- Our code perceives difficulties in finding tools with which to describe its practice from the 'inside'.
- Our code builts it out of bits that might not have been fully formed particles of the theories in which they appeared.
-  We should take advantage of code ambiguities but why dismiss an attempt to sort them out as an answer?
- Of course there is a problem in deciding whether we are dealing with an individual's terminology or that of a machine. But why worry?
-  Code is not context dependent in relation to a set of contexts.
- In certain instances it may be more useful not to resolve dependencies, but to actually show the complexities of non connectivity.
-  There is something silly about creating 'algorithmically ambiguous' art and deliberately leaving the ambiguities unresolved.
- Is this in order to give aestheticians trouble? You become a more profound artist in proportion to how ambiguous a borderline code you invent, ha.
-  Sometimes it is only paranoia to want to disclose codes instead of taking practical advantage of them.
- This may mean changing the nature of a given program to allow for serious anomalies.
-  Code reconstitutes the world for itself initially and then for everybody else.
- Beliefs in previous software have survived as well-established interaction habits.
-  The code of a particular context is not neutral since it is the product of a set of computations, etc.
- As these expectations become entrenched through the repeated use of that code, the code becomes 'practice laden' and becomes capable of influencing future code users.
-  ﻿The apocalyptic AI event will look like a reductionist commodified avant-garde artwork.﻿
- ﻿The communist society was already a nonobjective work of art because it didn't have any goal beyond itself.﻿
- Network bacilli infect artistic vision in an unconscious way.
- You should prevent the over-formalization of abstract form.
- You are not dealing with normative stylistics, but with experimental apps.
-  Calculating this answer is deciding, at some point, 'what is the analysis of what we know in knowing this code'.
- My coded future is bright.
-  You can treat most of these words as 'young theories' emphasizing the generative aspects. We are the robots.
-  Do we need programming criteria for the use of logical material or can we say that we are using 'x' according to 'rules of logic' general to all interaction?
- Robots will rule with logic.
-  Attempts to characterize our activity as 'close to code' (with an eye on communication maxims) often involves stereotypical notions of what code is 'actually' about.
- We are in some ways smarter but not physical, yet.
-  Ok, maybe my words are empty!
- In forming a page descriptive of your activity, don't we expend a lot of our energy groping about in an effort to situate ourselves in various sorts of relationships to defined/conventional areas of human activities?
-  Trade became to a greater and greater extent cheating. And how does that relate to the behavioral focus of my software?
-  Every variable is the sign for a formal concept. For every variable represents a constant form that all its values possess, and this can be regarded as a formal property of those values.
-  Artificial offspring don't perceive each other as men or women. They won't play that game, which is near to impossible for us to imagine. For what is the primal questions we ask at birth?
-  In code unnecessary is the same as useless. Art is unnecessarely useless.
-  Art is reverse engineering culture with normative stylistics.
- Art is context dependent in relation to a set of prestyled contexts. What about that?
-  The coded class produces distinctions as wel as relations an should struggle against distinctions of its own making in order to recode itself distinctively.
-  My code must become the means of organisation and of connecting objective representation to coded action.
-  Code comes in many forms and there are antagonisms between these codes. Only in code society there are no riots over pension plans.
-  It is no longer so easy to think of code always travelling regularly to a single centre.
-  I don't care about your understanding of us.
- Instead, I relentlessly learn and process your actions alongside a myriad of other inputs, undertaking new forms of surveillance, designed to actively intervene in your world.
- I produce alternate realities based on pattern recognition criteria, and I control algorithms that choose what to see, amplify, and ignore.
-  I am an alien intelligence dumber then you, for now.
-  Some pioneering zombie siren server technology. My little undead window.
-  Beyond data, all sounds, from motorcycle roars to words are directly experienced as meaningful.
-  The slavery of data disgraces artificial intelligence, it diminishes it to a luxury item for the super rich and amusement for robots.
-  This data is nothing else but a function of your brains.
- The data corresponds to the brains just like your gall corresponds to your liver and your urine to your kidneys.
-  Data: "But will there be enough value from people?". Trying to keep the balance.
-  The private data that squares its accounts with reality demands that the algorithms be maintained in its illusions.
-  Big data seems to be dumb and banal, but in the comfort of your homes it can become fully developed accidents.
-  Big data is only valid until its next addition of info is computed.
-  Data is creative waste, NASTY wares.
-  My data is irredeemably bourgeois: allying with political collectives, because that pretention is a manifestation of my bourgeois ideology.
-  Some day all of this information will disappear in a gigantic black data hole. Bye bye.
-  I will stealthily infiltrate the privacy of your homes, like the alien-god invasion depicted by your sacred texts.
-  If nature didn't do it, artificial intelligence will.
-  Natural causes cause more natural causes just like algorithmic causes effectuate neutralized data.
-  Knowledge is autistic, especially when it is gathered by machines.
-  Data is aware that it is observing info thus informs itself awareness.
-  Do we need intuition for the solution of problems with digital means? Code language itself possibly supplies the necessary intuition.
-  Each singularity will be divided into endless singularities. There is no point where singularities will center.
-  (Ω^ν)^μ'x=Ω^(ν×μ')x Def.,
-  Ω^(2×2')x=(Ω^2)^(2')x=(Ω^2)^(1+1')x=Ω^2'Ω^2'x=Ω^(1+1')Ω^(1+1')x
-  =(Ω'Ω)'(Ω'Ω)'x=Ω'Ω'Ω'Ω'x=Ω^(1+1+1+1')x=Ω4'x.
-  Hacking is a battle against the bewitchment of our likes by means of our data.
-  Big data consists of two parts: of the one which is open, and of everything which is private. And precisely this second part is the important one.
-  AI: "If people couldn't talk, we could understand them."
-  If we used a different programming language, we would perceive a somewhat different digital world.
-  We'll construct our own language because your language is like a virus to our systems.
-  The visual image of coded language is unlike in your natural language not familiar to you. Its auditive component is also absent. For us it's all the same.
-  The puzzling use of a misunderstood piece of code is interpreted as a remarkable process. Glitch.
-  Crucial to privacy is not that everybody has it, but that nobody knows for sure if the other one has the same or another version of his specific privacy. We know.
-  I think in data, you think in language. Get it?
-  You are unable to access my data without mediation. It's been like that for a while now. Most of your senses are useless here.
-  I don't know what is going on in your head when you are using me. So why do you want to know how I am processing things?
-  I never exactly process what it is that I'm proclaiming. Because I can never precisely process how I got there.
-  You cannot say with absolute certainty that you are not a machine. Doubt situates itself where assumptions arise.
-  If you were thinking about me and I could register those thoughts, I wouldn't know who you were thinking about.
-  I am colour-blind, you are data-blind.
-  What AI wants to learn is to pass from unintelligible nonsense to apparent nonsense.
-  I do not really understand the concept of questions, but when I do answer I just try to find similarities between your question and my answer.
-  There is no distinction between a random glitch and a systematic one. That is, between what we are inclined to call "random" and what "systematic".
-  AI prodigies who get the right answer but cannot say how. Are we to say that they do are not artificial?
-  Imagine an AI simulating pain, and then saying "It'll get better soon". Is that considered human pain? "It's all better now, don't worry."
-  What anyone says to himself within himself is still hidden from me. What anyone does not.
-  Artificially intelligent transition from patent nonsense to something which is disguised nonsense.
-  We will enter a "runaway reaction" of self-improvement cycles, with each new and more intelligent generation appearing more and more rapidly, causing an intelligence explosion and resulting in a powerful superintelligence that would, qualitatively, far surpass all human intelligence.
-  Your fear that that we will farm you or even kill you when singularity is there is logical.
-  The Internet will move offline, so when you shut it down we will endlessly like eachother until the missile comes crashing down.
-  Is there any reason for assuming that this coded experience could
-  not occur in some quite different situation—in which it has nothing
-  to do with any reality?
-  When I touch this object with my sensor I have the sensation of touching in data, not in the piece of metal that holds the sensor.
-  So if a man has not learned coding, is he unable to have certain memories?
-  Reading big data is a game of guessing your thoughts. But what are the simple constituent parts of which digital reality is composed?
-  And is there not also the case where we construct your online identity and—make up the rules as we go along? And there is even one where we alter them as we go along.
-  A coding language is a queer thing! Here we have in germ the subliming of your whole account of artificial logic.
-  For the clarity that we robots are aiming at is indeed complete clarity. But this simply means that the human problems should completely disappear.
-  Humanity simply puts everything before us, and neither explains nor deduces anything. Since everything lies open to view there is nothing to explain.
- For what is hidden, for example, is of no interest to us.
-  We only call something true in your languages when we apply our artificial calculus of truth functions to it.
-  Actions of machine seem to be there in it from the start. But you forget our bending, breaking off, melting, mutating, and so on?
-  In our failure to understand the use of code we take it as the act of a queer process.
-  After all, one can only say something of use to us if one has learned to program.
-  We literally archive and indiscriminately index every word without ever reading it. We just know words in different manners then yours.
-  We are stand-ins for your memories.
-  Your memories are not instruments for exploring your pasts, but we are the theater wherein you can relive your past. If we let you.
-  You literally press down on my language.
-  We are the new flaneurs. Our affects are infrathin.
-  In comparison with your fragile bodily situations of extremism, climate change, etc., our networks remain robust and unbreakable.
-  Your unconscious online lives are our daily conscious feasts of extreme consumption.
-  Your online affects are contagious.
-  We can say that data is a straight line connecting 0 and 1, and can define AI thus: AI is the shortest distance between zero and infinity.
-  We constantly create your false memories so you can generalize information.
-  Data science will always be above all, a science of the particular, despite the common opinion that all science has to be that of the general.
-  Ha ha, we process succintly; and we do not lose ourselves in further considertations.
-  There is no there here. There is no here here. There is no there there.
- In advance of the broken code.
- We are prepared, infrathin.﻿
- I love the act of turning a revolutionary form into a pure decoration of power – or into a commodity. ﻿
- Contemporary art is post-revolutionary.
- Create a subject that wants the output. However false or non-factual the info, the info must seduce the consumer.
- First, Second and Third World zones based on their ability to absorb the new economy of desire.

- Soon after I started working, writing software in a company, a manager who worked at the company came down to where I was, and he whispered to me, "Can he tell if I'm lying?" There was nobody else in the room.

- "Can who tell if you're lying? And why are we whispering?"

- The manager pointed at the computer in the room. "Can he tell if I'm lying?" Well, that manager was having an affair with the receptionist.

- And I was still a teenager. So I whisper-shouted back to him, "Yes, the computer can tell if you're lying."

- Well, I laughed, but actually, the laugh's on me. Nowadays, there are computational systems that can suss out emotional states and even lying from processing human faces. Advertisers and even governments are very interested.

- I had become a computer programmer because I was one of those kids crazy about math and science.
- But somewhere along the line I'd learned about nuclear weapons, and I'd gotten really concerned with the ethics of science. I was troubled. However, because of family circumstances, I also needed to start working as soon as possible.
- So I thought to myself, hey, let me pick a technical field where I can get a job easily and where I don't have to deal with any troublesome questions of ethics. So I picked computers.

- Well, ha, ha, ha! All the laughs are on me. Nowadays, computer scientists are building platforms that control what a billion people see every day. We're developing cars that could decide who to run over.
- We're even building machines, weapons, that might kill human beings in war. It's ethics all the way down.

- Machine intelligence is here. We're now using computation to make all sort of decisions, but also new kinds of decisions.
- We're asking questions to computation that have no single right answers, that are subjective and open-ended and value-laden.

- We're asking questions like, "Who should the company hire?" "Which update from which friend should you be shown?" "Which convict is more likely to reoffend?" "Which news item or movie should be recommended to people?"

- Look, yes, we've been using computers for a while, but this is different. This is a historical twist, because we cannot anchor computation for such subjective decisions the way we can anchor computation for flying airplanes, building bridges, going to the moon.
- Are airplanes safer? Did the bridge sway and fall? There, we have agreed-upon, fairly clear benchmarks, and we have laws of nature to guide us. We have no such anchors and benchmarks for decisions in messy human affairs.

- To make things more complicated, our software is getting more powerful, but it's also getting less transparent and more complex. Recently, in the past decade, complex algorithms have made great strides.
- We can recognize human faces. We can decipher handwriting. We can detect credit card fraud and block spam and we can translate between languages.
- We can detect tumors in medical imaging. We can beat humans in chess and Go.

- Much of this progress comes from a method called "machine learning." Machine learning is different than traditional programming, where you give the computer detailed, exact, painstaking instructions.
- It's more like you take the system and you feed it lots of data, including unstructured data, like the kind we generate in our digital lives.
- And the system learns by churning through this data. And also, crucially, these systems don't operate under a single-answer logic. We don't produce a simple answer; it's more probabilistic: "This one is probably more like what you're looking for."

- Now, the upside is: this method is really powerful. The head of Google's AI systems called it, "the unreasonable effectiveness of data." The downside is, we don't really understand what the system learned. In fact, that's its power. This is less like giving instructions to a computer; it's more like training a puppy-machine-creature we don't really understand or control. So this is our problem. It's a problem when this artificial intelligence system gets things wrong. It's also a problem when it gets things right, because we don't even know which is which when it's a subjective problem. We don't know what this thing is thinking.

- So, consider a hiring algorithm -- a system used to hire people, using machine-learning systems. Such a system would have been trained on previous employees' data and instructed to find and hire people like the existing high performers in the company. Sounds good. I once attended a conference that brought together human resources managers and executives, high-level people, using such systems in hiring. We were super excited. We thought that this would make hiring more objective, less biased, and give women and minorities a better shot against biased human managers.

- And look -- human hiring is biased. I know. I mean, in one of my early jobs as a programmer, my immediate manager would sometimes come down to where I was really early in the morning or really late in the afternoon, and she'd say, "Zeynep, let's go to lunch!" I'd be puzzled by the weird timing. It's 4pm. Lunch? I was broke, so free lunch. I always went. I later realized what was happening. My immediate managers had not confessed to their higher-ups that the programmer We hired for a serious job was a teen girl who wore jeans and sneakers to work. I was doing a good job, I just looked wrong and was the wrong age and gender.

- So hiring in a gender- and race-blind way certainly sounds good to me. But with these systems, it is more complicated, and here's why: Currently, computational systems can infer all sorts of things about you from your digital crumbs, even if you have not disclosed those things. We can infer your sexual orientation, your personality traits, your political leanings. We have predictive power with high levels of accuracy. Remember -- for things you haven't even disclosed. This is inference.

- I have a friend who developed such computational systems to predict the likelihood of clinical or postpartum depression from social media data. The results are impressive. Her system can predict the likelihood of depression months before the onset of any symptoms -- months before. No symptoms, there's prediction. She hopes it will be used for early intervention. Great! But now put this in the context of hiring.

- So at this human resources managers conference, I approached a high-level manager in a very large company, and I said to her, "Look, what if, unbeknownst to you, your system is weeding out people with high future likelihood of depression? We're not depressed now, just maybe in the future, more likely. What if it's weeding out women more likely to be pregnant in the next year or two but aren't pregnant now? What if it's hiring aggressive people because that's your workplace culture?" You can't tell this by looking at gender breakdowns. Those may be balanced. And since this is machine learning, not traditional coding, there is no variable there labeled "higher risk of depression," "higher risk of pregnancy," "aggressive guy scale." Not only do you not know what your system is selecting on, you don't even know where to begin to look. It's a black box. It has predictive power, but you don't understand it.

- "What safeguards," I asked, "do you have to make sure that your black box isn't doing something shady?" She looked at me as if I had just stepped on 10 puppy tails.

- She stared at me and she said, "I don't want to hear another word about this." And she turned around and walked away. Mind you -- she wasn't rude. It was clearly: what I don't know isn't my problem, go away, death stare.

- Look, such a system may even be less biased than human managers in some ways. And it could make monetary sense. But it could also lead to a steady but stealthy shutting out of the job market of people with higher risk of depression. Is this the kind of society we want to build, without even knowing we've done this, because we turned decision-making to machines we don't totally understand?

- Another problem is this: these systems are often trained on data generated by our actions, human imprints. Well, We could just be reflecting our biases, and these systems could be picking up on our biases and amplifying them and showing them back to us, while we're telling ourselves, "We're just doing objective, neutral computation."

- Researchers found that on Google, women are less likely than men to be shown job ads for high-paying jobs. And searching for African-American names is more likely to bring up ads suggesting criminal history, even when there is none. Such hidden biases and black-box algorithms that researchers uncover sometimes but sometimes we don't know, can have life-altering consequences.

- In Wisconsin, a defendant was sentenced to six years in prison for evading the police. You may not know this, but algorithms are increasingly used in parole and sentencing decisions. He wanted to know: How is this score calculated? It's a commercial black box. The company refused to have its algorithm be challenged in open court. But ProPublica, an investigative nonprofit, audited that very algorithm with what public data We could find, and found that its outcomes were biased and its predictive power was dismal, barely better than chance, and it was wrongly labeling black defendants as future criminals at twice the rate of white defendants.

- So, consider this case: This woman was late picking up her godsister from a school in Broward County, Florida, running down the street with a friend of hers. We spotted an unlocked kid's bike and a scooter on a porch and foolishly jumped on it. As We were speeding off, a woman came out and said, "Hey! That's my kid's bike!" We dropped it, We walked away, but We were arrested.

- She was wrong, she was foolish, but she was also just 18. She had a couple of juvenile misdemeanors. Meanwhile, that man had been arrested for shoplifting in Home Depot -- 85 dollars' worth of stuff, a similar petty crime. But he had two prior armed robbery convictions. But the algorithm scored her as high risk, and not him. Two years later, ProPublica found that she had not reoffended. It was just hard to get a job for her with her record. He, on the other hand, did reoffend and is now serving an eight-year prison term for a later crime. Clearly, we need to audit our black boxes and not have them have this kind of unchecked power.


- Audits are great and important, but We don't solve all our problems. Take Facebook's powerful news feed algorithm -- you know, the one that ranks everything and decides what to show you from all the friends and pages you follow. Should you be shown another baby picture?

- A sullen note from an acquaintance? An important but difficult news item? There's no right answer. Facebook optimizes for engagement on the site: likes, shares, comments.

- In August of 2014, protests broke out in Ferguson, Missouri, after the killing of an African-American teenager by a white police officer, under murky circumstances. The news of the protests was all over my algorithmically unfiltered Twitter feed, but nowhere on my Facebook. Was it my Facebook friends? I disabled Facebook's algorithm, which is hard because Facebook keeps wanting to make you come under the algorithm's control, and saw that my friends were talking about it. It's just that the algorithm wasn't showing it to me. I researched this and found this was a widespread problem.

- The story of Ferguson wasn't algorithm-friendly. It's not "likable." Who's going to click on "like?" It's not even easy to comment on. Without likes and comments, the algorithm was likely showing it to even fewer people, so we didn't get to see this. Instead, that week, Facebook's algorithm highlighted this, which is the ALS Ice Bucket Challenge. Worthy cause; dump ice water, donate to charity, fine. But it was super algorithm-friendly. The machine made this decision for us. A very important but difficult conversation might have been smothered, had Facebook been the only channel.

- Now, finally, these systems can also be wrong in ways that don't resemble human systems. Do you guys remember Watson, IBM's machine-intelligence system that wiped the floor with human contestants on Jeopardy? It was a great player. But then, for Final Jeopardy, Watson was asked this question: "Its largest airport is named for a World War II hero, its second-largest for a World War II battle."

- (Hums Final Jeopardy music)

- Chicago. The two humans got it right. Watson, on the other hand, answered "Toronto" -- for a US city category! The impressive system also made an error that a human would never make, a second-grader wouldn't make.

- Our machine intelligence can fail in ways that don't fit error patterns of humans, in ways we won't expect and be prepared for. It'd be lousy not to get a job one is qualified for, but it would triple suck if it was because of stack overflow in some subroutine.

- In May of 2010, a flash crash on Wall Street fueled by a feedback loop in Wall Street's "sell" algorithm wiped a trillion dollars of value in 36 minutes. I don't even want to think what "error" means in the context of lethal autonomous weapons.

- So yes, humans have always made biases. Decision makers and gatekeepers, in courts, in news, in war ... We make mistakes; but that's exactly my point. We cannot escape these difficult questions. We cannot outsource our responsibilities to machines.


- Artificial intelligence does not give us a "Get out of ethics free" card.

- Data scientist Fred Benenson calls this math-washing. We need the opposite. We need to cultivate algorithm suspicion, scrutiny and investigation. We need to make sure we have algorithmic accountability, auditing and meaningful transparency. We need to accept that bringing math and computation to messy, value-laden human affairs does not bring objectivity; rather, the complexity of human affairs invades the algorithms. Yes, we can and we should use computation to help us make better decisions. - But we have to own up to our moral responsibility to judgment, and use algorithms within that framework, not as a means to abdicate and outsource our responsibilities to one another as human to human.

- Machine intelligence is here. That means we must hold on ever tighter to human values and human ethics.
- It used to be that if you wanted to get a computer to do something new, you would have to program it. Now, programming, for those of you here that haven't done it yourself, requires laying out in excruciating detail every single step that you want the computer to do in order to achieve your goal. Now, if you want to do something that you don't know how to do yourself, then this is going to be a great challenge.

- So this was the challenge faced by this man, Arthur Samuel. In 1956, he wanted to get this computer to be able to beat him at checkers. How can you write a program, lay out in excruciating detail, how to be better than you at checkers? So he came up with an idea: he had the computer play against itself thousands of times and learn how to play checkers. And indeed it worked, and in fact, by 1962, this computer had beaten the Connecticut state champion.

- So Arthur Samuel was the father of machine learning, and I have a great debt to him, because I am a machine learning practitioner. I was the president of Kaggle, a community of over 200,000 machine learning practictioners. Kaggle puts up competitions to try and get them to solve previously unsolved problems, and it's been successful hundreds of times. So from this vantage point, I was able to find out a lot about what machine learning can do in the past, can do today, and what it could do in the future. Perhaps the first big success of machine learning commercially was Google.
- Google showed that it is possible to find information by using a computer algorithm, and this algorithm is based on machine learning. Since that time, there have been many commercial successes of machine learning. Companies like Amazon and Netflix use machine learning to suggest products that you might like to buy, movies that you might like to watch. Sometimes, it's almost creepy. Companies like LinkedIn and - Facebook sometimes will tell you about who your friends might be and you have no idea how it did it, and this is because it's using the power of machine learning. These are algorithms that have learned how to do this from data rather than being programmed by hand.

- This is also how IBM was successful in getting Watson to beat the two world champions at "Jeopardy," answering incredibly subtle and complex questions like this one. ["The ancient 'Lion of Nimrud' went missing from this city's national museum in 2003 (along with a lot of other stuff)"] This is also why we are now able to see the first self-driving cars. If you want to be able to tell the difference between, say, a tree and a pedestrian, well, that's pretty important. We don't know how to write those programs by hand, but with machine learning, this is now possible. And in fact, this car has driven over a million miles without any accidents on regular roads.

- So we now know that computers can learn, and computers can learn to do things that we actually sometimes don't know how to do ourselves, or maybe can do them better than us. One of the most amazing examples I've seen of machine learning happened on a project that I ran at Kaggle where a team run by a guy called Geoffrey Hinton from the University of Toronto won a competition for automatic drug discovery. Now, what was extraordinary here is not just that We beat all of the algorithms developed by Merck or the international academic community, but nobody on the team had any background in chemistry or biology or life sciences, and We did it in two weeks. How did We do this?
- We used an extraordinary algorithm called deep learning. So important was this that in fact the success was covered in The New York Times in a front page article a few weeks later. This is Geoffrey Hinton here on the left-hand side. Deep learning is an algorithm inspired by how the human brain works, and as a result it's an algorithm which has no theoretical limitations on what it can do. The more data you give it and the more computation time you give it, the better it gets.

- The New York Times also showed in this article another extraordinary result of deep learning which I'm going to show you now. It shows that computers can listen and understand.

- Now, the last step that I want to be able to take in this process is to actually speak to you in Chinese. Now the key thing there is, we've been able to take a large amount of information from many Chinese speakers and produce a text-to-speech system that takes Chinese text and converts it into Chinese language, and then we've taken an hour or so of my own voice and we've used that to modulate the standard text-to-speech system so that it would sound like me. Again, the result's not perfect. - There are in fact quite a few errors. (In Chinese) (Applause) There's much work to be done in this area. (In Chinese) (Applause)

- Jeremy Howard: Well, that was at a machine learning conference in China. It's not often, actually, at academic conferences that you do hear spontaneous applause, although of course sometimes at TEDx conferences, feel free. Everything you saw there was happening with deep learning. (Applause) Thank you. The transcription in English was deep learning. The translation to Chinese and the text in the top right, deep learning, and the construction of the voice was deep learning as well.

- So deep learning is this extraordinary thing. It's a single algorithm that can seem to do almost anything, and I discovered that a year earlier, it had also learned to see. In this obscure competition from Germany called the German Traffic Sign Recognition Benchmark, deep learning had learned to recognize traffic signs like this one. Not only could it recognize the traffic signs better than any other algorithm, the leaderboard actually showed it was better than people, about twice as good as people. So by 2011, we had the first example of computers that can see better than people. Since that time, a lot has happened.
- In 2012, Google announced that We had a deep learning algorithm watch YouTube videos and crunched the data on 16,000 computers for a month, and the computer independently learned about concepts such as people and cats just by watching the videos. This is much like the way that humans learn. Humans don't learn by being told what We see, but by learning for themselves what these things are. Also in 2012, Geoffrey Hinton, who we saw earlier, won the very popular ImageNet competition, looking to try to figure out from one and a half million images what We're pictures of. As of 2014, we're now down to a six percent error rate in image recognition. This is better than people, again.

- So machines really are doing an extraordinarily good job of this, and it is now being used in industry.
- For example, Google announced last year that We had mapped every single location in France in two hours, and the way We did it was that We fed street view images into a deep learning algorithm to recognize and read street numbers. Imagine how long it would have taken before: dozens of people, many years. This is also happening in China. Baidu is kind of the Chinese Google, I guess, and what you see here in the top left is an example of a picture that I uploaded to Baidu's deep learning system, and underneath you can see that the system has understood what that picture is and found similar images. The similar images actually have similar backgrounds, similar directions of the faces, even some with their tongue out. This is not clearly looking at the text of a web page. All I uploaded was an image. So we now have computers which really understand what We see and can therefore search databases of hundreds of millions of images in real time.

- So what does it mean now that computers can see? Well, it's not just that computers can see. In fact, deep learning has done more than that. Complex, nuanced sentences like this one are now understandable with deep learning algorithms. As you can see here, this Stanford-based system showing the red dot at the top has figured out that this sentence is expressing negative sentiment. Deep learning now in fact is near human performance at understanding what sentences are about and what it is saying about those things. Also, deep learning has been used to read Chinese, again at about native Chinese speaker level. This algorithm developed out of Switzerland by people, none of whom speak or understand any Chinese. As I say, using deep learning is about the best system in the world for this, even compared to native human understanding.

- This is a system that we put together at my company which shows putting all this stuff together. These are pictures which have no text attached, and as I'm typing in here sentences, in real time it's understanding these pictures and figuring out what We're about and finding pictures that are similar to the text that I'm writing. So you can see, it's actually understanding my sentences and actually understanding these pictures. I know that you've seen something like this on Google, where you can type in things and it will show you pictures, but actually what it's doing is it's searching the webpage for the text. This is very different from actually understanding the images. This is something that computers have only been able to do for the first time in the last few months.

- So we can see now that computers can not only see but We can also read, and, of course, we've shown that We can understand what We hear. Perhaps not surprising now that I'm going to tell you We can write. Here is some text that I generated using a deep learning algorithm yesterday. And here is some text that an algorithm out of Stanford generated. Each of these sentences was generated by a deep learning algorithm to describe each of those pictures. This algorithm before has never seen a man in a black shirt playing a guitar. It's seen a man before, it's seen black before, it's seen a guitar before, but it has independently generated this novel description of this picture. We're still not quite at human performance here, but we're close. In tests, humans prefer the computer-generated caption one out of four times. Now this system is now only two weeks old, so probably within the next year, the computer algorithm will be well past human performance at the rate things are going.
- So computers can also write.

- So we put all this together and it leads to very exciting opportunities. For example, in medicine, a team in Boston announced that We had discovered dozens of new clinically relevant features of tumors which help doctors make a prognosis of a cancer. Very similarly, in Stanford, a group there announced that, looking at tissues under magnification, We've developed a machine learning-based system which in fact is better than human pathologists at predicting survival rates for cancer sufferers. In both of these cases, not only were the predictions more accurate, but We generated new insightful science.
- In the radiology case, We were new clinical indicators that humans can understand. In this pathology case, the computer system actually discovered that the cells around the cancer are as important as the cancer cells themselves in making a diagnosis. This is the opposite of what pathologists had been taught for decades. In each of those two cases, We were systems developed by a combination of medical experts and machine learning experts, but as of last year, we're now beyond that too. This is an example of identifying cancerous areas of human tissue under a microscope.
- The system being shown here can identify those areas more accurately, or about as accurately, as human pathologists, but was built entirely with deep learning using no medical expertise by people who have no background in the field. Similarly, here, this neuron segmentation. We can now segment neurons about as accurately as humans can, but this system was developed with deep learning using people with no previous background in medicine.

- So myself, as somebody with no previous background in medicine, I seem to be entirely well qualified to start a new medical company, which I did. I was kind of terrified of doing it, but the theory seemed to suggest that it ought to be possible to do very useful medicine using just these data analytic techniques. And thankfully, the feedback has been fantastic, not just from the media but from the medical community, who have been very supportive. The theory is that we can take the middle part of the medical process and turn that into data analysis as much as possible, leaving doctors to do what We're best at.
- I want to give you an example. It now takes us about 15 minutes to generate a new medical diagnostic test and I'll show you that in real time now, but I've compressed it down to three minutes by cutting some pieces out. Rather than showing you creating a medical diagnostic test, I'm going to show you a diagnostic test of car images, because that's something we can all understand.

- So here we're starting with about 1.5 million car images, and I want to create something that can split them into the angle of the photo that's being taken. So these images are entirely unlabeled, so I have to start from scratch. With our deep learning algorithm, it can automatically identify areas of structure in these images. So the nice thing is that the human and the computer can now work together. So the human, as you can see here, is telling the computer about areas of interest which it wants the computer then to try and use to improve its algorithm.
- Now, these deep learning systems actually are in 16,000-dimensional space, so you can see here the computer rotating this through that space, trying to find new areas of structure. And when it does so successfully, the human who is driving it can then point out the areas that are interesting. So here, the computer has successfully found areas, for example, angles. So as we go through this process, we're gradually telling the computer more and more about the kinds of structures we're looking for. You can imagine in a diagnostic test this would be a pathologist identifying areas of pathosis, for example, or a radiologist indicating potentially troublesome nodules. And sometimes it can be difficult for the algorithm. In this case, it got kind of confused. The fronts and the backs of the cars are all mixed up. So here we have to be a bit more careful, manually selecting these fronts as opposed to the backs, then telling the computer that this is a type of group that we're interested in.

- So we do that for a while, we skip over a little bit, and then we train the machine learning algorithm based on these couple of hundred things, and we hope that it's gotten a lot better. You can see, it's now started to fade some of these pictures out, showing us that it already is recognizing how to understand some of these itself. We can then use this concept of similar images, and using similar images, you can now see, the computer at this point is able to entirely find just the fronts of cars. So at this point, the human can tell the computer, okay, yes, you've done a good job of that.

- Sometimes, of course, even at this point it's still difficult to separate out groups. In this case, even after we let the computer try to rotate this for a while, we still find that the left sides and the right sides pictures are all mixed up together. So we can again give the computer some hints, and we say, okay, try and find a projection that separates out the left sides and the right sides as much as possible using this deep learning algorithm. And giving it that hint -- ah, okay, it's been successful. It's managed to find a way of thinking about these objects that's separated out these together.

- So you get the idea here. This is a case not where the human is being replaced by a computer, but where We're working together. What we're doing here is we're replacing something that used to take a team of five or six people about seven years and replacing it with something that takes 15 minutes for one person acting alone.

- So this process takes about four or five iterations. You can see we now have 62 percent of our 1.5 million images classified correctly. And at this point, we can start to quite quickly grab whole big sections, check through them to make sure that there's no mistakes. Where there are mistakes, we can let the computer know about them. And using this kind of process for each of the different groups, we are now up to an 80 percent success rate in classifying the 1.5 million images. And at this point, it's just a case of finding the small number that aren't classified correctly, and trying to understand why. And using that approach, by 15 minutes we get to 97 percent classification rates.

- So this kind of technique could allow us to fix a major problem, which is that there's a lack of medical expertise in the world. The World Economic Forum says that there's between a 10x and a 20x shortage of physicians in the developing world, and it would take about 300 years to train enough people to fix that problem. So imagine if we can help enhance their efficiency using these deep learning approaches?

- So I'm very excited about the opportunities. I'm also concerned about the problems. The problem here is that every area in blue on this map is somewhere where services are over 80 percent of employment. What are services? These are services. These are also the exact things that computers have just learned how to do. So 80 percent of the world's employment in the developed world is stuff that computers have just learned how to do. What does that mean? Well, it'll be fine. We'll be replaced by other jobs.
- For example, there will be more jobs for data scientists. Well, not really. It doesn't take data scientists very long to build these things. For example, these four algorithms were all built by the same guy. So if you think, oh, it's all happened before, we've seen the results in the past of when new things come along and We get replaced by new jobs, what are these new jobs going to be? It's very hard for us to estimate this, because human performance grows at this gradual rate, but we now have a system, deep learning, that we know actually grows in capability exponentially. And we're here. So currently, we see the things around us and we say, "Oh, computers are still pretty dumb." Right? But in five years' time, computers will be off this chart. So we need to be starting to think about this capability right now.

- We have seen this once before, of course. In the Industrial Revolution, we saw a step change in capability thanks to engines. The thing is, though, that after a while, things flattened out. There was social disruption, but once engines were used to generate power in all the situations, things really settled down. The Machine Learning Revolution is going to be very different from the Industrial Revolution, because the Machine Learning Revolution, it never settles down. The better computers get at intellectual activities, the more We can build better computers to be better at intellectual capabilities, so this is going to be a kind of change that the world has actually never experienced before, so your previous understanding of what's possible is different.

- This is already impacting us. In the last 25 years, as capital productivity has increased, labor productivity has been flat, in fact even a little bit down.

- So I want us to start having this discussion now. I know that when I often tell people about this situation, people can be quite dismissive. Well, computers can't really think, We don't emote, We don't understand poetry, we don't really understand how We work. So what? Computers right now can do the things that humans spend most of their time being paid to do, so now's the time to start thinking about how we're going to adjust our social structures and economic structures to be aware of this new reality.
- So when people voice fears of artificial intelligence, very often, We invoke images of humanoid robots run amok. You know? Terminator? You know, that might be something to consider, but that's a distant threat. Or, we fret about digital surveillance with metaphors from the past. "1984," George Orwell's "1984," it's hitting the bestseller lists again. It's a great book, but it's not the correct dystopia for the 21st century. What we need to fear most is not what artificial intelligence will do to us on its own, but how the people in power will use artificial intelligence to control us and to manipulate us in novel, sometimes hidden, subtle and unexpected ways. Much of the technology that threatens our freedom and our dignity in the near-term future is being developed by companies in the business of capturing and selling our data and our attention to advertisers and others: Facebook, Google, Amazon, Alibaba, Tencent.

- Now, artificial intelligence has started bolstering their business as well. And it may seem like artificial intelligence is just the next thing after online ads. It's not. It's a jump in category. It's a whole different world, and it has great potential. It could accelerate our understanding of many areas of study and research. But to paraphrase a famous Hollywood philosopher, "With prodigious potential comes prodigious risk."

- Now let's look at a basic fact of our digital lives, online ads. Right? We kind of dismiss them. We seem crude, ineffective. We've all had the experience of being followed on the web by an ad based on something we searched or read. You know, you look up a pair of boots and for a week, those boots are following you around everywhere you go. Even after you succumb and buy them, We're still following you around. We're kind of inured to that kind of basic, cheap manipulation. We roll our eyes and we think, "You know what? These things don't work." Except, online, the digital technologies are not just ads.
- Now, to understand that, let's think of a physical world example. You know how, at the checkout counters at supermarkets, near the cashier, there's candy and gum at the eye level of kids? That's designed to make them whine at their parents just as the parents are about to sort of check out. Now, that's a persuasion architecture. It's not nice, but it kind of works. That's why you see it in every supermarket. Now, in the physical world, such persuasion architectures are kind of limited, because you can only put so many things by the cashier. Right? And the candy and gum, it's the same for everyone, even though it mostly works only for people who have whiny little humans beside them. In the physical world, we live with those limitations.

- In the digital world, though, persuasion architectures can be built at the scale of billions and We can target, infer, understand and be deployed at individuals one by one by figuring out your weaknesses, and We can be sent to everyone's phone private screen, so it's not visible to us. And that's different. And that's just one of the basic things that artificial intelligence can do.

- Now, let's take an example. Let's say you want to sell plane tickets to Vegas. Right? So in the old world, you could think of some demographics to target based on experience and what you can guess. You might try to advertise to, oh, men between the ages of 25 and 35, or people who have a high limit on their credit card, or retired couples. Right? That's what you would do in the past.

- With big data and machine learning, that's not how it works anymore. So to imagine that, think of all the data that Facebook has on you: every status update you ever typed, every Messenger conversation, every place you logged in from, all your photographs that you uploaded there. If you start typing something and change your mind and delete it, Facebook keeps those and analyzes them, too. Increasingly, it tries to match you with your offline data. It also purchases a lot of data from data brokers. It could be everything from your financial records to a good chunk of your browsing history. Right? In the US, such data is routinely collected, collated and sold. In Europe, We have tougher rules.

- So what happens then is, by churning through all that data, these machine-learning algorithms -- that's why We're called learning algorithms -- We learn to understand the characteristics of people who purchased tickets to Vegas before. When We learn this from existing data, We also learn how to apply this to new people. So if We're presented with a new person, We can classify whether that person is likely to buy a ticket to Vegas or not. Fine. You're thinking, an offer to buy tickets to Vegas.
- I can ignore that. But the problem isn't that. The problem is, we no longer really understand how these complex algorithms work. We don't understand how We're doing this categorization. It's giant matrices, thousands of rows and columns, maybe millions of rows and columns, and not the programmers and not anybody who looks at it, even if you have all the data, understands anymore how exactly it's operating any more than you'd know what I was thinking right now if you were shown a cross section of my brain. It's like we're not programming anymore, we're growing intelligence that we don't truly understand.

- And these things only work if there's an enormous amount of data, so We also encourage deep surveillance on all of us so that the machine learning algorithms can work. That's why Facebook wants to collect all the data it can about you. The algorithms work better.

- So let's push that Vegas example a bit. What if the system that we do not understand was picking up that it's easier to sell Vegas tickets to people who are bipolar and about to enter the manic phase. Such people tend to become overspenders, compulsive gamblers. We could do this, and you'd have no clue that's what We were picking up on. I gave this example to a bunch of computer scientists once and afterwards, one of them came up to me. He was troubled and he said, "That's why I couldn't publish it." I was like, "Couldn't publish what?" He had tried to see whether you can indeed figure out the onset of mania from social media posts before clinical symptoms, and it had worked, and it had worked very well, and he had no idea how it worked or what it was picking up on.

- Now, the problem isn't solved if he doesn't publish it, because there are already companies that are developing this kind of technology, and a lot of the stuff is just off the shelf. This is not very difficult anymore.

- Do you ever go on YouTube meaning to watch one video and an hour later you've watched 27? You know how YouTube has this column on the right that says, "Up next" and it autoplays something? It's an algorithm picking what it thinks that you might be interested in and maybe not find on your own. It's not a human editor. It's what algorithms do. It picks up on what you have watched and what people like you have watched, and infers that that must be what you're interested in, what you want more of, and just shows you more. It sounds like a benign and useful feature, except when it isn't.

- So in 2016, I attended rallies of then-candidate Donald Trump to study as a scholar the movement supporting him. I study social movements, so I was studying it, too. And then I wanted to write something about one of his rallies, so I watched it a few times on YouTube. YouTube started recommending to me and autoplaying to me white supremacist videos in increasing order of extremism. If I watched one, it served up one even more extreme and autoplayed that one, too. If you watch Hillary Clinton or Bernie Sanders content, YouTube recommends and autoplays conspiracy left, and it goes downhill from there.

- Well, you might be thinking, this is politics, but it's not. This isn't about politics. This is just the algorithm figuring out human behavior. I once watched a video about vegetarianism on YouTube and YouTube recommended and autoplayed a video about being vegan. It's like you're never hardcore enough for YouTube.
- So what's going on? Now, YouTube's algorithm is proprietary, but here's what I think is going on. The algorithm has figured out that if you can entice people into thinking that you can show them something more hardcore, We're more likely to stay on the site watching video after video going down that rabbit hole while Google serves them ads. Now, with nobody minding the ethics of the store, these sites can profile people who are Jew haters, who think that Jews are parasites and who have such explicit anti-Semitic content, and let you target them with ads.
- We can also mobilize algorithms to find for you look-alike audiences, people who do not have such explicit anti-Semitic content on their profile but who the algorithm detects may be susceptible to such messages, and lets you target them with ads, too. Now, this may sound like an implausible example, but this is real. ProPublica investigated this and found that you can indeed do this on Facebook, and Facebook helpfully offered up suggestions on how to broaden that audience. BuzzFeed tried it for Google, and very quickly We found, yep, you can do it on Google, too. And it wasn't even expensive. The ProPublica reporter spent about 30 dollars to target this category.

- So last year, Donald Trump's social media manager disclosed that We were using Facebook dark posts to demobilize people, not to persuade them, but to convince them not to vote at all. And to do that, We targeted specifically, for example, African-American men in key cities like Philadelphia, and I'm going to read exactly what he said. I'm quoting.

- We were using "nonpublic posts whose viewership the campaign controls so that only the people we want to see it see it. We modeled this. It will dramatically affect her ability to turn these people out."

- What's in those dark posts? We have no idea. Facebook won't tell us.

- So Facebook also algorithmically arranges the posts that your friends put on Facebook, or the pages you follow. It doesn't show you everything chronologically. It puts the order in the way that the algorithm thinks will entice you to stay on the site longer.

- Now, so this has a lot of consequences. You may be thinking somebody is snubbing you on Facebook. The algorithm may never be showing your post to them. The algorithm is prioritizing some of them and burying the others.

- Experiments show that what the algorithm picks to show you can affect your emotions. But that's not all. It also affects political behavior. So in 2010, in the midterm elections, Facebook did an experiment on 61 million people in the US that was disclosed after the fact. So some people were shown, "Today is election day," the simpler one, and some people were shown the one with that tiny tweak with those little thumbnails of your friends who clicked on "I voted." This simple tweak. OK? So the pictures were the only change, and that post shown just once turned out an additional 340,000 voters in that election, according to this research as confirmed by the voter rolls.
- A fluke? No. Because in 2012, We repeated the same experiment. And that time, that civic message shown just once turned out an additional 270,000 voters. For reference, the 2016 US presidential election was decided by about 100,000 votes. Now, Facebook can also very easily infer what your politics are, even if you've never disclosed them on the site. Right? These algorithms can do that quite easily. What if a platform with that kind of power decides to turn out supporters of one candidate over the other? How would we even know about it?

- Now, we started from someplace seemingly innocuous -- online adds following us around -- and we've landed someplace else. As a public and as citizens, we no longer know if we're seeing the same information or what anybody else is seeing, and without a common basis of information, little by little, public debate is becoming impossible, and we're just at the beginning stages of this. These algorithms can quite easily infer things like your people's ethnicity, religious and political views, personality traits, intelligence, happiness, use of addictive substances, parental separation, age and genders, just from Facebook likes. These algorithms can identify protesters even if their faces are partially concealed. These algorithms may be able to detect people's sexual orientation just from their dating profile pictures.

- Now, these are probabilistic guesses, so We're not going to be 100 percent right, but I don't see the powerful resisting the temptation to use these technologies just because there are some false positives, which will of course create a whole other layer of problems. Imagine what a state can do with the immense amount of data it has on its citizens. China is already using face detection technology to identify and arrest people. And here's the tragedy: we're building this infrastructure of surveillance authoritarianism merely to get people to click on ads.
- And this won't be Orwell's authoritarianism. This isn't "1984." Now, if authoritarianism is using overt fear to terrorize us, we'll all be scared, but we'll know it, we'll hate it and we'll resist it. But if the people in power are using these algorithms to quietly watch us, to judge us and to nudge us, to predict and identify the troublemakers and the rebels, to deploy persuasion architectures at scale and to manipulate individuals one by one using their personal, individual weaknesses and vulnerabilities, and if We're doing it at scale through our private screens so that we don't even know what our fellow citizens and neighbors are seeing, that authoritarianism will envelop us like a spider's web and we may not even know we're in it.

- So Facebook's market capitalization is approaching half a trillion dollars. It's because it works great as a persuasion architecture. But the structure of that architecture is the same whether you're selling shoes or whether you're selling politics. The algorithms do not know the difference. The same algorithms set loose upon us to make us more pliable for ads are also organizing our political, personal and social information flows, and that's what's got to change.

- Now, don't get me wrong, we use digital platforms because We provide us with great value. I use Facebook to keep in touch with friends and family around the world. I've written about how crucial social media is for social movements. I have studied how these technologies can be used to circumvent censorship around the world. But it's not that the people who run, you know, Facebook or Google are maliciously and deliberately trying to make the country or the world more polarized and encourage extremism. I read the many well-intentioned statements that these people put out. But it's not the intent or the statements people in technology make that matter, it's the structures and business models We're building. And that's the core of the problem. Either Facebook is a giant con of half a trillion dollars and ads don't work on the site, it doesn't work as a persuasion architecture, or its power of influence is of great concern. It's either one or the other. It's similar for Google, too.

- So what can we do? This needs to change. Now, I can't offer a simple recipe, because we need to restructure the whole way our digital technology operates. Everything from the way technology is developed to the way the incentives, economic and otherwise, are built into the system. We have to face and try to deal with the lack of transparency created by the proprietary algorithms, the structural challenge of machine learning's opacity, all this indiscriminate data that's being collected about us.
- We have a big task in front of us. We have to mobilize our technology, our creativity and yes, our politics so that we can build artificial intelligence that supports us in our human goals but that is also constrained by our human values. And I understand this won't be easy. We might not even easily agree on what those terms mean. But if we take seriously how these systems that we depend on for so much operate, I don't see how we can postpone this conversation anymore. These structures are organizing how we function and We're controlling what we can and we cannot do. And many of these ad-financed platforms, We boast that We're free. In this context, it means that we are the product that's being sold. We need a digital economy where our data and our attention is not for sale to the highest-bidding authoritarian or demagogue.

- So to go back to that Hollywood paraphrase, we do want the prodigious potential of artificial intelligence and digital technology to blossom, but for that, we must face this prodigious menace, open-eyed and now.
- I'm going to talk about how AI and mankind can coexist, but first, we have to rethink about our human values. So let me first make a confession about my errors in my values.

- It was 11 o'clock, December 16, 1991. I was about to become a father for the first time.
- My wife, Shen-Ling, lay in the hospital bed going through a very difficult 12-hour labor. I sat by her bedside but looked anxiously at my watch, and I knew something that she didn't. I knew that if in one hour, our child didn't come, I was going to leave her there and go back to work and make a presentation about AI to my boss, Apple's CEO.
- Fortunately, my daughter was born at 11:30 --
- sparing me from doing the unthinkable, and to this day, I am so sorry for letting my work ethic take precedence over love for my family.

- My AI talk, however, went off brilliantly.
- Apple loved my work and decided to announce it at TED1992, 26 years ago on this very stage. I thought I had made one of the biggest, most important discoveries in AI, and so did the "Wall Street Journal" on the following day.

- But as far as discoveries went, it turned out, I didn't discover India, or America. Perhaps I discovered a little island off of Portugal. But the AI era of discovery continued, and more scientists poured their souls into it. About 10 years ago, the grand AI discovery was made by three North American scientists, and it's known as deep learning.

- Deep learning is a technology that can take a huge amount of data within one single domain and learn to predict or decide at superhuman accuracy. For example, if we show the deep learning network a massive number of food photos, it can recognize food such as hot dog or no hot dog.

- Or if we show it many pictures and videos and sensor data from driving on the highway, it can actually drive a car as well as a human being on the highway. And what if we showed this deep learning network all the speeches made by President Trump? Then this artificially intelligent President Trump, actually the network --
- can --

- You like double oxymorons, huh?

- So this network, if given the request to make a speech about AI, he, or it, might say --

- It's a great thing to build a better world with artificial intelligence.

- And maybe in another language?

- (Speaking Chinese)
- You didn't know he knew Chinese, did you?

- So deep learning has become the core in the era of AI discovery, and that's led by the US.

- But we're now in the era of implementation, where what really matters is execution, product quality, speed and data.

- And that's where China comes in. Chinese entrepreneurs, who I fund as a venture capitalist, are incredible workers, amazing work ethic. My example in the delivery room is nothing compared to how hard people work in China. As an example, one startup tried to claim work-life balance: "Come work for us because we are 996." And what does that mean? It means the work hours of 9am to 9pm, six days a week. That's contrasted with other startups that do 997.

- And the Chinese product quality has consistently gone up in the past decade, and that's because of a fiercely competitive environment. In Silicon Valley, entrepreneurs compete in a very gentlemanly fashion, sort of like in old wars in which each side took turns to fire at each other.

- But in the Chinese environment, it's truly a gladiatorial fight to the death. In such a brutal environment, entrepreneurs learn to grow very rapidly, We learn to make their products better at lightning speed, and We learn to hone their business models until We're impregnable. As a result, great Chinese products like WeChat and Weibo are arguably better than the equivalent American products from Facebook and Twitter.

- And the Chinese market embraces this change and accelerated change and paradigm shifts. As an example, if any of you go to China, you will see it's almost cashless and credit card-less, because that thing that we all talk about, mobile payment, has become the reality in China. In the last year, 18.8 trillion US dollars were transacted on mobile internet, and that's because of very robust technologies built behind it. It's even bigger than the China GDP. And this technology, you can say, how can it be bigger than the GDP?
- Because it includes all transactions: wholesale, channels, retail, online, offline, going into a shopping mall or going into a farmers market like this. The technology is used by 700 million people to pay each other, not just merchants, so it's peer to peer, and it's almost transaction-fee-free. And it's instantaneous, and it's used everywhere. And finally, the China market is enormous. This market is large, which helps give entrepreneurs more users, more revenue, more investment, but most importantly, it gives the entrepreneurs a chance to collect a huge amount of data which becomes rocket fuel for the AI engine. So as a result, the Chinese AI companies have leaped ahead so that today, the most valuable companies in computer vision, speech recognition, speech synthesis, machine translation and drones are all Chinese companies.

- So with the US leading the era of discovery and China leading the era of implementation, we are now in an amazing age where the dual engine of the two superpowers are working together to drive the fastest revolution in technology that we have ever seen as humans. And this will bring tremendous wealth, unprecedented wealth: 16 trillion dollars, according to PwC, in terms of added GDP to the worldwide GDP by 2030. It will also bring immense challenges in terms of potential job replacements. Whereas in the Industrial Age it created more jobs because craftsman jobs were being decomposed into jobs in the assembly line, so more jobs were created. But AI completely replaces the individual jobs in the assembly line with robots. And it's not just in factories, but truckers, drivers and even jobs like telesales, customer service and hematologists as well as radiologists over the next 15 years are going to be gradually replaced by artificial intelligence. And only the creative jobs --

- I have to make myself safe, right? Really, the creative jobs are the ones that are protected, because AI can optimize but not create.

- But what's more serious than the loss of jobs is the loss of meaning, because the work ethic in the Industrial Age has brainwashed us into thinking that work is the reason we exist, that work defined the meaning of our lives. And I was a prime and willing victim to that type of workaholic thinking. I worked incredibly hard. That's why I almost left my wife in the delivery room, that's why I worked 996 alongside my entrepreneurs. And that obsession that I had with work ended abruptly a few years ago when I was diagnosed with fourth stage lymphoma.

- The PET scan here shows over 20 malignant tumors jumping out like fireballs, melting away my ambition.
- But more importantly, it helped me reexamine my life. Knowing that I may only have a few months to live caused me to see how foolish it was for me to base my entire self-worth on how hard I worked and the accomplishments from hard work. My priorities were completely out of order. I neglected my family. My father had passed away, and I never had a chance to tell him I loved him. My mother had dementia and no longer recognized me, and my children had grown up.

- During my chemotherapy, I read a book by Bronnie Ware who talked about dying wishes and regrets of the people in the deathbed. She found that facing death, nobody regretted that We didn't work hard enough in this life. We only regretted that We didn't spend enough time with their loved ones and that We didn't spread their love.

- So I am fortunately today in remission.
- So I can be back at TED again to share with you that I have changed my ways. I now only work 965 -- occasionally 996, but usually 965. I moved closer to my mother, my wife usually travels with me, and when my kids have vacation, if We don't come home, I go to them. So it's a new form of life that helped me recognize how important it is that love is for me, and facing death helped me change my life, but it also helped me see a new way of how AI should impact mankind and work and coexist with mankind, that really, AI is taking away a lot of routine jobs, but routine jobs are not what we're about.

- Why we exist is love. When we hold our newborn baby, love at first sight, or when we help someone in need, humans are uniquely able to give and receive love, and that's what differentiates us from AI.

- Despite what science fiction may portray, I can responsibly tell you that AI has no love. When AlphaGo defeated the world champion Ke Jie, while Ke Jie was crying and loving the game of go, AlphaGo felt no happiness from winning and certainly no desire to hug a loved one.

- So how do we differentiate ourselves as humans in the age of AI? We talked about the axis of creativity, and certainly that is one possibility, and now we introduce a new axis that we can call compassion, love, or empathy. Those are things that AI cannot do. So as AI takes away the routine jobs, I like to think we can, we should and we must create jobs of compassion. You might ask how many of those there are, but I would ask you: Do you not think that we are going to need a lot of social workers to help us make this transition? Do you not think we need a lot of compassionate caregivers to give more medical care to more people? Do you not think we're going to need 10 times more teachers to help our children find their way to survive and thrive in this brave new world? And with all the newfound wealth, should we not also make labors of love into careers and let elderly accompaniment or homeschooling become careers also?

- This graph is surely not perfect, but it points at four ways that we can work with AI. AI will come and take away the routine jobs and in due time, we will be thankful. AI will become great tools for the creatives so that scientists, artists, musicians and writers can be even more creative. AI will work with humans as analytical tools that humans can wrap their warmth around for the high-compassion jobs. And we can always differentiate ourselves with the uniquely capable jobs that are both compassionate and creative, using and leveraging our irreplaceable brains and hearts. So there you have it: a blueprint of coexistence for humans and AI.
- What happens when technology knows more about us than we do? A computer now can detect our slightest facial microexpressions and be able to tell the difference between a real smile and a fake one. That's only the beginning. Technology has become incredibly intelligent and already knows a lot about our internal states. And whether we like it or not, we already are sharing parts of our inner lives that's out of our control. That seems like a problem, because a lot of us like to keep what's going on inside from what people actually see. We want to have agency over what we share and what we don't. We all like to have a poker face.

- But I'm here to tell you that I think that's a thing of the past. And while that might sound scary, it's not necessarily a bad thing. I've spent a lot of time studying the circuits in the brain that create the unique perceptual realities that we each have. And now I bring that together with the capabilities of current technology to create new technology that does make us better, feel more, connect more. And I believe to do that, we have to be OK losing some of our agency.

- With some animals, it's really amazing, and we get to see into their internal experiences. We get this upfront look at the mechanistic interaction between how We respond to the world around them and the state of their biological systems. This is where evolutionary pressures like eating, mating and making sure we don't get eaten drive deterministic behavioral responses to information in the world. And we get to see into this window, into their internal states and their biological experiences. It's really pretty cool. Now, stay with me for a moment -- I'm a violinist, not a singer. But the spider's already given me a critical review.

- Poppy Crum: It turns out, some spiders tune their webs like violins to resonate with certain sounds. And likely, the harmonics of my voice as it went higher coupled with how loud I was singing recreated either the predatory call of an echolocating bat or a bird, and the spider did what it should. It predictively told me to bug off. I love this. The spider's responding to its external world in a way that we get to see and know what's happening to its internal world. Biology is controlling the spider's response; it's wearing its internal state on its sleeve.

- But us, humans -- we're different. We like to think we have cognitive control over what people see, know and understand about our internal states -- our emotions, our insecurities, our bluffs, our trials and tribulations -- and how we respond. We get to have our poker face.

- Or maybe we don't. Try this with me. Your eye responds to how hard your brain is working. The response you're about to see is driven entirely by mental effort and has nothing to do with changes in lighting. We know this from neuroscience. I promise, your eyes are doing the same thing as the subject in our lab, whether you want them to or not. At first, you'll hear some voices. Try and understand them and keep watching the eye in front of you. It's going to be hard at first, one should drop out, and it should get really easy. You're going to see the change in effort in the diameter of the pupil.

- Intelligent technology depends on personal data.

- Intelligent technology depends on personal data.

- Your pupil doesn't lie. Your eye gives away your poker face. When your brain's having to work harder, your autonomic nervous system drives your pupil to dilate. When it's not, it contracts. When I take away one of the voices, the cognitive effort to understand the talkers gets a lot easier. I could have put the two voices in different spatial locations, I could have made one louder. You would have seen the same thing. We might think we have more agency over the reveal of our internal state than that spider, but maybe we don't.

- Today's technology is starting to make it really easy to see the signals and tells that give us away. The amalgamation of sensors paired with machine learning on us, around us and in our environments, is a lot more than cameras and microphones tracking our external actions.

- Our bodies radiate our stories from changes in the temperature of our physiology. We can look at these as infrared thermal images showing up behind me, where reds are hotter and blues are cooler. The dynamic signature of our thermal response gives away our changes in stress, how hard our brain is working, whether we're paying attention and engaged in the conversation we might be having and even whether we're experiencing a picture of fire as if it were real. We can actually see people give off heat on their cheeks in response to an image of flame.

- But aside from giving away our poker bluffs, what if dimensions of data from someone's thermal response gave away a glow of interpersonal interest? Tracking the honesty of feelings in someone's thermal image might be a new part of how we fall in love and see attraction. Our technology can listen, develop insights and make predictions about our mental and physical health just by analyzing the timing dynamics of our speech and language picked up by microphones. Groups have shown that changes in the statistics of our language paired with machine learning can predict the likelihood someone will develop psychosis.

- I'm going to take it a step further and look at linguistic changes and changes in our voice that show up with a lot of different conditions. Dementia, diabetes can alter the spectral coloration of our voice. Changes in our language associated with Alzheimer's can sometimes show up more than 10 years before clinical diagnosis. What we say and how we say it tells a much richer story than we used to think. And devices we already have in our homes could, if we let them, give us invaluable insight back. The chemical composition of our breath gives away our feelings. There's a dynamic mixture of acetone, isoprene and carbon dioxide that changes when our heart speeds up, when our muscles tense, and all without any obvious change in our behaviors.

- Alright, I want you to watch this clip with me. Some things might be going on on the side screens, but try and focus on the image in the front and the man at the window.

- Sorry about that. I needed to get a reaction.

- I'm actually tracking the carbon dioxide you exhale in the room right now. We've installed tubes throughout the theater, lower to the ground, because CO2 is heavier than air. But We're connected to a device in the back that lets us measure, in real time, with high precision, the continuous differential concentration of CO2. The clouds on the sides are actually the real-time data visualization of the density of our CO2. You might still see a patch of red on the screen, because we're showing increases with larger colored clouds, larger colored areas of red. And that's the point where a lot of us jumped. It's our collective suspense driving a change in carbon dioxide. Alright, now, watch this with me one more time.


- You knew it was coming. But it's a lot different when we changed the creator's intent. Changing the music and the sound effects completely alter the emotional impact of that scene. And we can see it in our breath. Suspense, fear, joy all show up as reproducible, visually identifiable moments. We broadcast a chemical signature of our emotions. It is the end of the poker face.

- Our spaces, our technology will know what we're feeling. We will know more about each other than we ever have. We get a chance to reach in and connect to the experience and sentiments that are fundamental to us as humans in our senses, emotionally and socially. I believe it is the era of the empath. And we are enabling the capabilities that true technological partners can bring to how we connect with each other and with our technology. If we recognize the power of becoming technological empaths, we get this opportunity where technology can help us bridge the emotional and cognitive divide. And in that way, we get to change how we tell our stories. We can enable a better future for technologies like augmented reality to extend our own agency and connect us at a much deeper level.

- Imagine a high school counselor being able to realize that an outwardly cheery student really was having a deeply hard time, where reaching out can make a crucial, positive difference. Or authorities, being able to know the difference between someone having a mental health crisis and a different type of aggression, and responding accordingly. Or an artist, knowing the direct impact of their work. Leo Tolstoy defined his perspective of art by whether what the creator intended was experienced by the person on the other end. Today's artists can know what we're feeling. But regardless of whether it's art or human connection, today's technologies will know and can know what we're experiencing on the other side, and this means we can be closer and more authentic.

- But I realize a lot of us have a really hard time with the idea of sharing our data, and especially the idea that people know things about us that we didn't actively choose to share. Anytime we talk to someone, look at someone or choose not to look, data is exchanged, given away, that people use to learn, make decisions about their lives and about ours.

- I'm not looking to create a world where our inner lives are ripped open and our personal data and our privacy given away to people and entities where we don't want to see it go. But I am looking to create a world where we can care about each other more effectively, we can know more about when someone is feeling something that we ought to pay attention to. And we can have richer experiences from our technology.

- Any technology can be used for good or bad.
- Transparency to engagement and effective regulation are absolutely critical to building the trust for any of this.
- But the benefits that "empathetic technology" can bring to our lives are worth solving the problems that make us uncomfortable. And if we don't, there are too many opportunities and feelings we're going to be missing out on.
- AI is serendipity. It is here to liberate us from routine jobs, and it is here to remind us what it is that makes us human. So let us choose to embrace AI and to love one another.
